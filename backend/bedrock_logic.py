import boto3
import json
import os
import requests
import re
from bs4 import BeautifulSoup
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from backend.routes.ai_challenge_router import AICallengeCreateRequest, create_and_join_ai_challenge
from backend.routes.dashboard import get_dashboard # Import get_dashboard
from backend.database import get_db
from backend.dependencies import get_current_user
from sqlalchemy.orm import Session
from backend import crud # crud ëª¨ë“ˆ ì„í¬íŠ¸
from backend.models import User, TransportMode, Challenge, ChallengeMember # User ëª¨ë¸ ì„í¬íŠ¸

# --- ì„¤ì • ---
AWS_DEFAULT_REGION = "us-east-1"
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyBgs37kJYWB7zsTfIrDTqe1hpOxBhNkH44") # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "01354cc88406341ec") # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ë³€ê²½
BEDROCK_MODEL_ARN = os.getenv("BEDROCK_MODEL_ARN", "arn:aws:bedrock:us-east-1:327784329358:inference-profile/us.anthropic.claude-opus-4-20250514-v1:0")
BEDROCK_KNOWLEDGE_BASE_ID = os.getenv("BEDROCK_KNOWLEDGE_BASE_ID", "PUGB1AL6L1")

# --- Boto3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=AWS_DEFAULT_REGION)
    bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=AWS_DEFAULT_REGION)
    print("[ì•Œë¦¼] AWS Bedrock í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"[ì˜¤ë¥˜] AWS í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ, ê´€ë ¨ í•¨ìˆ˜ë“¤ì´ ConnectionErrorë¥¼ ë°œìƒì‹œí‚¤ë„ë¡ í•¨

# FastAPI ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/chat",
    tags=["Chatbot"]
)

class ChatRequest(BaseModel):
    user_id: int
    message: str

def invoke_llm(system_prompt, user_prompt):
    """
    ë²”ìš© Bedrock LLM í˜¸ì¶œ í•¨ìˆ˜
    """
    if not bedrock_runtime_client:
        raise ConnectionError("Bedrock runtime client is not initialized.")
    try:
        messages = [{"role": "user", "content": user_prompt}]
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4096,
            "system": system_prompt,
            "messages": messages
        }
        response = bedrock_runtime_client.invoke_model(
            modelId=BEDROCK_MODEL_ARN,
            body=json.dumps(request_body)
        )
        response_body = json.loads(response.get('body').read())
        return response_body['content'][0]['text']
    except Exception as e:
        print(f"Bedrock ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def query_knowledge_base(query):
    """
    Bedrock ì§€ì‹ ê¸°ë°˜ì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ê³¼ ì¶œì²˜ë¥¼ ë°›ì•„ì˜¤ëŠ” í•¨ìˆ˜
    """
    if not bedrock_agent_runtime_client:
        raise ConnectionError("Bedrock agent runtime client is not initialized.")
    print(f"\n[ì•Œë¦¼] Bedrock ì§€ì‹ ê¸°ë°˜ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    try:
        response = bedrock_agent_runtime_client.retrieve_and_generate(
            input={'text': query},
            retrieveAndGenerateConfiguration={
                'type': 'KNOWLEDGE_BASE',
                'knowledgeBaseConfiguration': {
                    'knowledgeBaseId': BEDROCK_KNOWLEDGE_BASE_ID,
                    'modelArn': BEDROCK_MODEL_ARN
                }
            }
        )
        
        if response and response.get('output') and response.get('citations'):
            answer = response['output']['text']
            citations = response['citations']
            
            source_details = []
            for citation in citations:
                if citation.get('retrievedReferences'):
                    retrieved_ref = citation['retrievedReferences'][0]
                    location = retrieved_ref.get('location', {}).get('s3Location', {}).get('uri')
                    if location:
                        source_details.append(f"- {location}")

            if source_details:
                formatted_answer = f"{answer}\n\n--- ì¶œì²˜ ---\n" + "\n".join(source_details)
            else:
                formatted_answer = answer

            print("[ì•Œë¦¼] ì§€ì‹ ê¸°ë°˜ì—ì„œ ë‹µë³€ì„ ì„±ê³µì ìœ¼ë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return formatted_answer
        else:
            print("[ì•Œë¦¼] ì§€ì‹ ê¸°ë°˜ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        print(f"Bedrock ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def perform_web_search(query):
    """
    Google ê²€ìƒ‰ìœ¼ë¡œ URLì„ ì°¾ê³ , ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ(ì œëª©->ë³¸ë¬¸) ì‹¤ì œ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    print(f"\n[ì•Œë¦¼] ì›¹ì—ì„œ '{query}'ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    try:
        search_url = "https://www.googleapis.com/customsearch/v1"
        search_params = {'key': GOOGLE_API_KEY, 'cx': GOOGLE_CSE_ID, 'q': query, 'num': 3}
        search_response = requests.get(search_url, params=search_params)
        search_response.raise_for_status()
        search_results = search_response.json()
        items = search_results.get('items', [])

        if not items:
            return "ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        full_context = ""
        urls = [item.get('link') for item in items]
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/555.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/555.36'} # User-Agent ìˆ˜ì •

        print("[ì•Œë¦¼] ê²€ìƒ‰ëœ ì›¹í˜ì´ì§€ì˜ ë³¸ë¬¸ì„ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        for url in urls:
            if not url: continue
            try:
                page_response = requests.get(url, headers=headers, timeout=5)
                page_response.raise_for_status()
                soup = BeautifulSoup(page_response.text, 'lxml')
                text_parts = []
                tags_to_extract = ['h1', 'h2', 'h3', 'p']
                for tag in tags_to_extract:
                    elements = soup.find_all(tag)
                    for element in elements:
                        text_parts.append(element.get_text(strip=True))
                page_text = '\n'.join(text_parts)
                full_context += f"--- URL: {url}ì˜ ë‚´ìš© ---\n{page_text}\n\n"
            except requests.exceptions.RequestException as e:
                print(f"  - URL {url} ë°©ë¬¸ ì‹¤íŒ¨: {e}")
                continue

        if not full_context:
            return "ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

        print(f"[ì¶”ì¶œ ì™„ë£Œ] ì´ {len(full_context)}ìë¦¬ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        return full_context

    except Exception as e:
        print(f"ì›¹ ê²€ìƒ‰ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return "ì •ë³´ ê²€ìƒ‰ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@router.post("/")
async def chatbot_endpoint(request: ChatRequest):
    user_query = request.message
    user_id = request.user_id # user_idëŠ” í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ì¶”í›„ í™•ì¥ì„±ì„ ìœ„í•´ ìœ ì§€

    print(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n")

    print("[1ë‹¨ê³„] ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤...")
    router_system_prompt = f"""
    You are a smart orchestrator that analyzes the user's question and decides which action to take.
    You must choose one of the following six actions and respond only in JSON format. Do not add any other explanations.

    1. "knowledge_base_search": Choose this when the user's question is likely to be answered by a private knowledge base, containing specific information about people, projects, or internal documents. Specific topics like 'carbon reduction' or 'recycling' can also belong here.
       - Example: {{"action": "knowledge_base_search", "query": "information about ecomileage-seoul"}}
       - Example: {{"action": "knowledge_base_search", "query": "how to recycle plastic"}}

    2. "general_search": Choose this when the user's question requires up-to-date information or general knowledge that is unlikely to be in a private knowledge base, such as 'today's weather' or 'news about celebrities'.
       - Example: {{"action": "general_search", "query": "today's weather in Seoul"}}

    3. "recommend_challenge": Choose this when the user explicitly asks for a challenge recommendation or expresses a desire to start a new eco-friendly challenge.
       - Example: {{"action": "recommend_challenge", "user_intent": "recommend an eco-friendly challenge"}}
       - Example: {{"action": "recommend_challenge", "user_intent": "I want to start a new challenge"}}

    4. "get_carbon_reduction_tip": Choose this when the user asks for tips on reducing carbon emissions or eco-friendly practices.
       - Example: {{"action": "get_carbon_reduction_tip", "user_intent": "give me a tip for reducing carbon"}}
       - Example: {{"action": "get_carbon_reduction_tip", "user_intent": "how can I be more eco-friendly?"}}

    5. "get_goal_strategy": Choose this when the user asks for strategies to achieve their eco-friendly goals or improve their progress.
       - Example: {{"action": "get_goal_strategy", "user_intent": "how can I achieve my carbon reduction goal?"}}
       - Example: {{"action": "get_goal_strategy", "user_intent": "give me a strategy to improve my eco-score"}}

    6. "direct_answer": Choose this for simple greetings or small talk, like "Hello", "Thank you", or "Who are you?".
       - Example: {{"action": "direct_answer", "answer": "Hello! I am an AI chatbot here to answer your questions. Ask me anything!"}}

    7. "detect_activity_and_suggest_challenge": Choose this when the user mentions a specific eco-friendly action they have taken, like "I walked to work" or "I took the bus today". This is different from asking for a recommendation.
       - Example: {{"action": "detect_activity_and_suggest_challenge", "activity": "rode a bike"}}

    User question: "{user_query}"
    Your JSON response:
    """
    
    router_output_str = invoke_llm(router_system_prompt, user_query)
    
    action = None
    router_decision = {}
    if router_output_str is None:
        print("[ì˜¤ë¥˜] Bedrock ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: router_output_strì´ Noneì…ë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        action = "general_search"
        router_decision = {"query": user_query}
    else:
        try:
            json_match = re.search(r'\{.*\}', router_output_str, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                router_decision = json.loads(json_str)
                action = router_decision.get("action")
            else:
                raise ValueError("No JSON object found in the router output")
        except (json.JSONDecodeError, ValueError, AttributeError, ConnectionError) as e:
            print(f"[ì˜¤ë¥˜] ì¡°ìœ¨ì(Router)ì˜ ê²°ì •ì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ë˜ëŠ” Bedrock ì—°ê²° ì˜¤ë¥˜: {e}. ì¼ë°˜ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            action = "general_search"
            router_decision = {"query": user_query}

    final_answer = ""
    query = router_decision.get("query", user_query)
    original_action = action

    if action == "knowledge_base_search":
        print(f"[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: '{action}'. ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        final_answer = query_knowledge_base(query)
        
        if not final_answer:
            print("[ì•Œë¦¼] ì§€ì‹ ê¸°ë°˜ì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            action = "general_search"

    if action == "general_search":
        if original_action == 'knowledge_base_search':
            pass
        else:
            print(f"[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: '{action}'. ì›¹ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        search_results = perform_web_search(query)

        if "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in search_results or "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in search_results:
            final_answer = search_results
        else:
            max_context_length = 20000
            if len(search_results) > max_context_length:
                print(f"\n[ì•Œë¦¼] ì¶”ì¶œëœ ì •ë³´ê°€ ë„ˆë¬´ ê¸¸ì–´({len(search_results)}ì), í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•˜ë„ë¡ {max_context_length}ìë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤.")
                search_results = search_results[:max_context_length]
            
            print("\n[3ë‹¨ê³„] ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            final_answer_system_prompt = """
            ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ì¹œì ˆí•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ë‹µë³€ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼(<search_results>) ì•ˆì˜ ì •ë³´ì—ë§Œ ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
            ì ˆëŒ€ë¡œ ë‹¹ì‹ ì˜ ê¸°ì¡´ ì§€ì‹ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì •ë³´ë¥¼ ì¶”ì¸¡í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
            ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ê²€ìƒ‰ëœ ì •ë³´ë¡œëŠ” ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ë¼ê³ ë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.
            """
            
            if original_action == "general_search":
                final_answer_system_prompt += "\n\në‹µë³€ì„ ë§ˆì¹œ í›„, ë§ˆì§€ë§‰ì—ëŠ” **ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ì™€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë‚´ìš©ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬** ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ëŠ” í™˜ê²½ ë³´í˜¸ë‚˜ íƒ„ì†Œ ì ˆê° ê´€ë ¨ ì œì•ˆì„ í•œ ë¬¸ì¥ ë§ë¶™ì—¬ì£¼ì„¸ìš”. \n    ì˜ˆë¥¼ ë“¤ì–´, ë‚ ì”¨ê°€ ì¢‹ë‹¤ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ ìì „ê±° íƒ€ê¸°ë¥¼ ì¶”ì²œí•˜ê³ , ë¯¸ì„¸ë¨¼ì§€ê°€ ë§ë‹¤ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ ëŒ€ì¤‘êµí†µ ì´ìš©ì„ ì¶”ì²œí•˜ê³ , íƒ„ì†Œ ì ˆê° ê´€ë ¨ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¼ìƒìƒí™œì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” íŒì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

            final_answer = invoke_llm(final_answer_system_prompt, f"<search_results>\n{search_results}\n</search_results>\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_query}")

    elif action == "detect_activity_and_suggest_challenge":
        print("[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: 'detect_activity_and_suggest_challenge'. í™œë™ ê°ì§€ ë° ì±Œë¦°ì§€ ì¶”ì²œ/ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        db = next(get_db())
        
        # í™œë™ í‚¤ì›Œë“œ ë° í•´ë‹¹ TransportMode ë§¤í•‘
        activity_keywords = {
            "ìì „ê±°": TransportMode.BIKE,
            "ê±¸ì–´ì„œ": TransportMode.WALK,
            "ë„ë³´": TransportMode.WALK,
            "ë²„ìŠ¤": TransportMode.BUS,
            "ì§€í•˜ì² ": TransportMode.SUBWAY,
        }
        
        detected_activity_mode = None
        detected_keyword = None
        for keyword, mode in activity_keywords.items():
            if keyword in user_query:
                detected_activity_mode = mode
                detected_keyword = keyword
                break

        if not detected_activity_mode:
            # ê´€ë ¨ í™œë™ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ ë‹µë³€ìœ¼ë¡œ ì „í™˜
            final_answer = invoke_llm("You are a friendly AI assistant.", user_query)
            return {"response": final_answer}

        # ì‚¬ìš©ìì˜ ì˜¤ëŠ˜ í™œë™ ê¸°ë¡ í™•ì¸ (Task 2.3)
        from datetime import date, timedelta
        today_start = date.today()
        # KST ê³ ë ¤ (UTC+9)
        utc_today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(hours=9)
        if datetime.utcnow().hour < 9: # UTC ê¸°ì¤€ 00:00-08:59ëŠ” í•œêµ­ì˜ ê°™ì€ ë‚ 
             utc_today_start -= timedelta(days=1)
        utc_today_end = utc_today_start + timedelta(days=1)


        mobility_log = db.query(models.MobilityLog).filter(
            models.MobilityLog.user_id == user_id,
            models.MobilityLog.transport_mode == detected_activity_mode,
            models.MobilityLog.started_at >= utc_today_start,
            models.MobilityLog.started_at < utc_today_end
        ).order_by(models.MobilityLog.started_at.desc()).first()

        if mobility_log:
            # True Case (2.3): í™œë™ ê¸°ë¡ì´ ìˆì„ ê²½ìš°
            print(f"[ì•Œë¦¼] ì‚¬ìš©ìì˜ '{detected_activity_mode.value}' í™œë™ ê¸°ë¡ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
            
            # ì¶”ê°€ í¬ë ˆë”§ ì§€ê¸‰ ë¡œì§ (ì˜ˆ: ê±°ë¦¬ 1kmë‹¹ 5 í¬ë ˆë”§)
            bonus_credits = int(mobility_log.distance_km * 5)
            
            # í¬ë ˆë”§ ì¶”ê°€
            crud.create_credit_log(db, user_id=user_id, points=bonus_credits, reason=f"ì±—ë´‡ í™œë™ í™•ì¸ ë³´ë„ˆìŠ¤: {detected_keyword}")
            
            response_text = f"ë„¤! ì˜¤ëŠ˜ {mobility_log.distance_km:.1f}kmë¥¼ {detected_keyword}(ìœ¼)ë¡œ ì´ë™í•˜ì‹  ê¸°ë¡ì„ í™•ì¸í–ˆì–´ìš”. ì •ë§ ë©‹ì ¸ìš”! ì¶”ê°€ ë³´ë„ˆìŠ¤ë¡œ {bonus_credits}Cë¥¼ ë“œë ¸ìŠµë‹ˆë‹¤. ğŸ"
            final_answer = response_text
            return {"response": final_answer}

        else:
            # False Case (2.3) / Challenge Suggestion (2.2): í™œë™ ê¸°ë¡ì´ ì—†ì„ ê²½ìš°
            print(f"[ì•Œë¦¼] ì‚¬ìš©ìì˜ '{detected_activity_mode.value}' í™œë™ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ê´€ë ¨ ì±Œë¦°ì§€ë¥¼ ì°¾ì•„ë´…ë‹ˆë‹¤.")
            
            # ì°¸ì—¬ ê°€ëŠ¥í•œ ê´€ë ¨ ì±Œë¦°ì§€ ê²€ìƒ‰
            joined_challenge_ids = {m.challenge_id for m in db.query(models.ChallengeMember).filter(models.ChallengeMember.user_id == user_id).all()}
            
            available_challenges = db.query(models.Challenge).filter(
                models.Challenge.challenge_id.notin_(joined_challenge_ids),
                models.Challenge.title.contains(detected_keyword)
            ).all()

            if available_challenges:
                # ê´€ë ¨ ì±Œë¦°ì§€ê°€ ìˆì„ ê²½ìš° ì œì•ˆ
                suggested_challenge = available_challenges[0]
                
                suggestion_prompt = f'''
                You are a friendly and encouraging AI assistant.
                A user mentioned they did an activity: "{user_query}".
                Your task is to naturally praise their action and suggest the following challenge.
                Keep the response concise and friendly.
                
                Challenge to suggest:
                - Title: {suggested_challenge.title}
                - Description: {suggested_challenge.description}
                
                Your response should end with a question asking if they want to join.
                Example: "ìì „ê±°ë¥¼ íƒ€ì…¨êµ°ìš”! ì •ë§ ì¢‹ì€ ìŠµê´€ì´ì—ìš”. í˜¹ì‹œ '{suggested_challenge.title}'ì— ì°¸ì—¬í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
                '''
                
                response_text = invoke_llm(suggestion_prompt, "")
                
                return {{
                    "response": response_text,
                    "suggestion": {{
                        "type": "challenge",
                        "challenge_id": suggested_challenge.challenge_id,
                        "title": suggested_challenge.title
                    }}
                }}
            else:
                # ê´€ë ¨ ì±Œë¦°ì§€ê°€ ì—†ì„ ê²½ìš°
                final_answer = f"ì•„, ê·¸ëŸ¬ì…¨êµ°ìš”! ì•„ì‰½ê²Œë„ ì˜¤ëŠ˜ {detected_keyword} ì´ë™ ê¸°ë¡ì´ í™•ì¸ë˜ì§€ ì•Šë„¤ìš”. ì´ë™ ê¸°ë¡ì´ ìˆì–´ì•¼ ë³´ë„ˆìŠ¤ í¬ë ˆë”§ì„ ë°›ì„ ìˆ˜ ìˆì–´ìš”."
                return {{"response": final_answer}}

    elif action == "recommend_challenge":
        print("[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: 'recommend_challenge'. AI ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ì‚¬ìš©ì ëŒ€ì‹œë³´ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        db_session = next(get_db())
        current_user_obj = db_session.query(User).filter(User.user_id == user_id).first()
        if not current_user_obj:
            raise HTTPException(status_code=404, detail="User not found for challenge recommendation.")
        
        dashboard_data = await get_dashboard(current_user=current_user_obj, db=db_session)
        
        # LLMì—ê²Œ ì±Œë¦°ì§€ ì•„ì´ë””ì–´ë¥¼ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        challenge_prompt = f"""
        You are an AI assistant that generates eco-friendly challenge ideas.
        Based on the user's intent and their recent activity data, generate a single challenge idea in JSON format.
        The challenge should be simple, actionable, and encourage carbon reduction.
        Prioritize light challenges that the user hasn't done much recently, or suggest new types of activities.
        Avoid recommending challenges for activities the user has frequently done in the last 7 days.
        
        User's recent activity data:
        - Last 7 days carbon saved (g): {json.dumps([{{"date": str(d.date), "saved_g": d.saved_g}} for d in dashboard_data.last7days])}
        - Mode statistics: {json.dumps([{{"mode": m.mode, "saved_g": m.saved_g}} for m in dashboard_data.modeStats])}
        - Total carbon saved (kg): {dashboard_data.total_saved}
        - Current garden level: {dashboard_data.garden_level}
        
        Provide a title, a short description, a reward (integer, e.g., 100), a goal_type (CO2_SAVED, DISTANCE_KM, TRIP_COUNT), a goal_target_value (float), and optionally a target_mode (ANY, WALK, BIKE, PUBLIC_TRANSPORT).
        If no specific mode is implied, use default values.
        
        Example JSON format for a light challenge:
        {{
            "title": "ë¶„ë¦¬ìˆ˜ê±° ì±Œë¦°ì§€",
            "description": "ì˜¤ëŠ˜ í•˜ë£¨ ë¶„ë¦¬ìˆ˜ê±°ë¥¼ ì™„ë²½í•˜ê²Œ ì‹¤ì²œí•´ ë³´ì„¸ìš”!",
            "reward": 20,
            "target_mode": "ANY"
        }}
        Example JSON format for another light challenge:
        {{
            "title": "ìƒ¤ì›Œ 10ë¶„ ì±Œë¦°ì§€",
            "description": "ìƒ¤ì›Œ ì‹œê°„ì„ 10ë¶„ ì´ë‚´ë¡œ ì¤„ì—¬ ë¬¼ê³¼ ì—ë„ˆì§€ë¥¼ ì ˆì•½í•´ ë³´ì„¸ìš”!",
            "reward": 30,
            "target_mode": "ANY"
        }}
        
        User intent: "{router_decision.get("user_intent", user_query)}"
        Your JSON response:
        """
        
        challenge_idea_str = invoke_llm(challenge_prompt, "")
        
        try:
            challenge_idea = json.loads(challenge_idea_str)
            
            # AICallengeCreateRequest ëª¨ë¸ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„
            challenge_request = AICallengeCreateRequest(
                title=challenge_idea.get("title", "AI ì¶”ì²œ ì±Œë¦°ì§€"),
                description=challenge_idea.get("description", "AIê°€ ì¶”ì²œí•˜ëŠ” ì¹œí™˜ê²½ ì±Œë¦°ì§€ì…ë‹ˆë‹¤."),
                reward=challenge_idea.get("reward", 30),
                target_mode=TransportMode[challenge_idea.get("target_mode", "ANY").upper()] if challenge_idea.get("target_mode") else TransportMode.ANY,
                goal_type=schemas.ChallengeGoalType[challenge_idea.get("goal_type", "CO2_SAVED").upper()],
                goal_target_value=challenge_idea.get("goal_target_value", 1000.0)
            )
            
            # ì‹¤ì œ User ê°ì²´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ (ì´ë¯¸ ìœ„ì—ì„œ current_user_objë¡œ ê°€ì ¸ì˜´)
            
            challenge_response = await create_and_join_ai_challenge(
                request=challenge_request,
                db=db_session,
                current_user=current_user_obj # ì‹¤ì œ User ê°ì²´ ì „ë‹¬
            )
            
            final_answer = challenge_response.get("message", "AI ì±Œë¦°ì§€ ìƒì„± ë° ì°¸ì—¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            if challenge_response.get("challenge"):
                final_answer += f" ì±Œë¦°ì§€ ì œëª©: {challenge_response['challenge'].title}"
            
        except json.JSONDecodeError as e:
            print(f"[ì˜¤ë¥˜] AI ì±Œë¦°ì§€ ì•„ì´ë””ì–´ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            final_answer = "AI ì±Œë¦°ì§€ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"[ì˜¤ë¥˜] AI ì±Œë¦°ì§€ ìƒì„± ë° ì°¸ì—¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            final_answer = f"AI ì±Œë¦°ì§€ ìƒì„± ë° ì°¸ì—¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    elif action == "get_carbon_reduction_tip":
        print("[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: 'get_carbon_reduction_tip'. íƒ„ì†Œ ì ˆê° íŒì„ ìƒì„±í•©ë‹ˆë‹¤.")
        tip_system_prompt = f"""
        You are an AI assistant that provides concise and actionable tips for carbon reduction and eco-friendly practices.
        Generate a single, practical tip based on the user's intent.
        The tip should be encouraging and easy to understand.
        """
        final_answer = invoke_llm(tip_system_prompt, router_decision.get("user_intent", user_query))

    elif action == "get_goal_strategy":
        print("[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: 'get_goal_strategy'. ëª©í‘œ ë‹¬ì„± ì „ëµì„ ìƒì„±í•©ë‹ˆë‹¤.")
        strategy_system_prompt = f"""
        You are an AI assistant that provides effective strategies for achieving eco-friendly goals and improving progress.
        Generate a single, actionable strategy based on the user's intent.
        The strategy should be motivating and provide clear steps.
        """
        final_answer = invoke_llm(strategy_system_prompt, router_decision.get("user_intent", user_query))

    elif action == "direct_answer":
        print("[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: 'direct_answer'. ì¦‰ì‹œ ë‹µë³€í•©ë‹ˆë‹¤.")
        final_answer = router_decision.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if not final_answer:
        final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    print("\n--- ìµœì¢… ë‹µë³€ ---")
    print(final_answer)
    return {"response": final_answer}